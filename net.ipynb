{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f51cb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openseespy.opensees as ops\n",
    "import openseespy.postprocessing.ops_vis as opsv\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pylab as plt\n",
    "import cv2\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "### Splitting to test, validation and train 80%/10%/10%\n",
    "train = pd.read_hdf('train_norm.h5')\n",
    "validation = pd.read_hdf('val_norm.h5')\n",
    "test = pd.read_hdf('test_norm.h5')\n",
    "\n",
    "### Splitting to input X and target y sets\n",
    "### Converting to torch Tensor format (for torch trainig)\n",
    "X_train = train[['P', 'My', 'Mz', 'fc', 'h']]\n",
    "y_train = train[[ 'Width', 'Depth','As_total']]#, 'D_rebar']]\n",
    "\n",
    "X_val = validation[['P', 'My', 'Mz', 'fc', 'h']]\n",
    "y_val = validation[[ 'Width', 'Depth','As_total']]#, 'D_rebar']]\n",
    "\n",
    "X_test = test[['P',  'My', 'Mz', 'fc', 'h']]\n",
    "y_test = test[['Width', 'Depth','As_total']]#, 'D_rebar']]\n",
    "\n",
    "### Converting to torch Tensor format (for torch trainig)\n",
    "\n",
    "X_train = torch.from_numpy(X_train.to_numpy()).float()\n",
    "y_train = torch.from_numpy(y_train.to_numpy()).float()\n",
    "\n",
    "X_val = torch.from_numpy(X_val.to_numpy()).float()\n",
    "y_val = torch.from_numpy(y_val.to_numpy()).float()\n",
    "\n",
    "\n",
    "X_test = torch.from_numpy(X_test.to_numpy()).float()\n",
    "y_test = torch.from_numpy(y_test.to_numpy()).float()\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "val_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,      # torch TensorDataset format\n",
    "    batch_size=batch_size,  \n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset=val_dataset,      # torch TensorDataset format\n",
    "    batch_size=batch_size,      # mini batch size\n",
    ")\n",
    "\n",
    "\n",
    "#LeakyReLu    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self,n_in, n_out, neurons):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Linear(n_in, 128),\n",
    "          #nn.BatchNorm1d(neurons),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, neurons),\n",
    "          #nn.BatchNorm1d(neurons),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(neurons, neurons),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(neurons, neurons),\n",
    "          #nn.BatchNorm1d(neurons),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(neurons, neurons),\n",
    "          #nn.BatchNorm1d(neurons),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(neurons, 512),\n",
    "          #nn.BatchNorm1d(neurons),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(512, 512),\n",
    "          #nn.BatchNorm1d(neurons),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(512, 512),\n",
    "          #nn.BatchNorm1d(neurons),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(512, neurons),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(neurons, neurons),\n",
    "          #nn.BatchNorm1d(neurons),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(neurons, neurons),\n",
    "          #nn.BatchNorm1d(neurons),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(neurons, neurons),\n",
    "          #nn.BatchNorm1d(neurons),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(neurons, 128),\n",
    "          #nn.BatchNorm1d(neurons),\n",
    "          nn.Dropout(p=0.1),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, n_out)\n",
    "    )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "net = Net(X_train.shape[1], y_train.shape[1], 256)\n",
    "print(net)\n",
    "\n",
    "#optimizer = optim.RMSprop(net.parameters(), lr=0.1, weight_decay=1e-4) \n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "device = torch.device(\"cuda:8\" if torch.cuda.is_available() else \"cpu\") #torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "net = net.to(device)\n",
    "loss_func = nn.MSELoss()  \n",
    "#loss_func = loss_func.to(device) \n",
    "\n",
    "# initialize a dictionary to store training history\n",
    "H = {\"train_loss\": [], \"val_loss\": []}\n",
    "epochs=150\n",
    "\n",
    "#scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "# calculate steps per epoch for training and validation set\n",
    "trainSteps = len(train_loader.dataset) // batch_size\n",
    "valSteps = len(val_loader.dataset) // batch_size\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # set the model in training mode\n",
    "    net.train()\n",
    "    # initialize the total training and validation loss\n",
    "    totalTrainLoss = 0\n",
    "    totalValLoss = 0\n",
    "    total_loss = 0\n",
    "    # loop over the training set\n",
    "    for i,batch in enumerate(train_loader):\n",
    "        #torch.cuda.empty_cache()\n",
    "        x_train,y_target = batch\n",
    "        \n",
    "        # send the input to the device\n",
    "        x_train, y_target = x_train.to(device),y_target.to(device)\n",
    "        y_pred = net(x_train)\n",
    "        \n",
    "        train_loss = loss_func(y_pred, y_target)\n",
    "    \n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        train_loss.backward()   # backpropagation, compute gradients\n",
    "        optimizer.step()        # apply gradients\n",
    "        #scheduler.step()\n",
    "\n",
    "        totalTrainLoss += train_loss \n",
    "    print(\"Epoch # %i, train_loss=%f\"%(epoch, totalTrainLoss))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # set the model in evaluation mode\n",
    "        net.eval()\n",
    "        for i,batch in enumerate(val_loader):\n",
    "            x_val,y_val = batch\n",
    "            \n",
    "            x_val, y_val = x_val.to(device),y_val.to(device)\n",
    "            pred = net(x_val)\n",
    "            totalValLoss += loss_func(pred, y_val)\n",
    "    \n",
    "    # calculate the average training and validation loss\n",
    "    avgTrainLoss = totalTrainLoss / trainSteps\n",
    "    avgValLoss = totalValLoss / valSteps\n",
    "\n",
    "    # update our training history\n",
    "    H[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "    H[\"val_loss\"].append(avgValLoss.cpu().detach().numpy())\n",
    "\n",
    "### Saving the model\n",
    "MODEL_PATH = 'fc_30.pth' \n",
    "torch.save(net, MODEL_PATH)\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(H[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(H[\"val_loss\"], label=\"val_loss\")\n",
    "plt.title(\"Training and validation lossess\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.savefig(str(batch_size)+'_lr= '+str(learning_rate)+'_dp=0.3'+'_.png')\n",
    "\n",
    "net.eval()\n",
    "\n",
    "outp = net(X_test.to(device))\n",
    "\n",
    "outp = outp.cpu()\n",
    "out = outp.detach().numpy()\n",
    "nn_out = pd.DataFrame(out, columns = ['Width', 'Depth', 'As_total'])\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "train_min = pd.read_hdf(\"train_min.h5\")\n",
    "train_min = train_min.drop(['P', 'My', 'Mz', 'fc', 'h'])\n",
    "\n",
    "train_max = pd.read_hdf(\"train_max.h5\")\n",
    "train_max = train_max.drop(['P', 'My', 'Mz', 'fc', 'h'])\n",
    "\n",
    "test_orig = pd.read_hdf(\"test.h5\")\n",
    "test_orig = test_orig.drop(columns=['P','My', 'Mz', 'fc', 'h'])\n",
    "\n",
    "\n",
    "back_scaled_nn = train_min + nn_out*(train_max - train_min) \n",
    "\n",
    "error_scaled = mean_squared_error(back_scaled_nn, test_orig, multioutput='raw_values')\n",
    "\n",
    "error = mean_squared_error(nn_out,y_test, multioutput='raw_values')\n",
    "print(error_scaled)\n",
    "print(error)\n",
    "\n",
    "#[1.49517889e-03 1.35164139e-03 2.67062261e-06]\n",
    "#[0.00233622 0.00211194 0.00361142]\n",
    "## nn latest\n",
    "#[1.70006513e-03 1.60931905e-03 2.43812537e-06]\n",
    "#[0.00265635 0.00251456 0.00329702]\n",
    "\n",
    "#[1.28692248e-03 1.26065387e-03 1.80810405e-06]\n",
    "#[0.00201082 0.00196977 0.00244506]\n",
    "\n",
    "a_file = open(\"Losses.csv\", \"w\")\n",
    "\n",
    "\n",
    "writer = csv.writer(a_file)\n",
    "for key, value in H.items():\n",
    "    writer.writerow([key, value])\n",
    "\n",
    "a_file.close()\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d6e1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = H['train_loss']\n",
    "val_loss = H['val_loss']\n",
    "df = pd.DataFrame(columns=['train loss', 'vall loss'])\n",
    "df['train loss'] = train_loss\n",
    "df['vall loss'] = val_loss\n",
    "df.to_csv(\"loss_curve.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59b33b8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "net.eval()\n",
    "\n",
    "test_samples = pd.read_hdf(\"test_samples.h5\")\n",
    "X_test = torch.from_numpy(test_samples.to_numpy()).float()\n",
    "\n",
    "outp = net(X_test.to(device))\n",
    "\n",
    "outp = outp.cpu()\n",
    "out = outp.detach().numpy()\n",
    "nn_out = pd.DataFrame(out, columns = ['Width', 'Depth', 'As_total'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "train_min = pd.read_hdf(\"train_min.h5\")\n",
    "train_min = train_min.drop(['P', 'My', 'Mz', 'fc', 'h'])\n",
    "\n",
    "train_max = pd.read_hdf(\"train_max.h5\")\n",
    "train_max = train_max.drop(['P', 'My', 'Mz', 'fc', 'h'])\n",
    "\n",
    "\n",
    "back_scaled_nn = train_min + nn_out*(train_max - train_min)\n",
    "\n",
    "\n",
    "back_scaled_nn.to_csv(\"nn_combo.csv\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "percent"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
